[[analytics-overview]]
== Analytics

:linkattrs:

=== Overview

Analytics embody algorithms tailored to geospatial data.  Most analytics leverage Hadoop MapReduce for bulk computation. Results of analytic jobs consist of vector or raster data stored in GeoWave.  The analytics infrastructure provides tools to build algorithms in Spark. For example, a Kryo serializer/deserializer enables exchange of SimpleFeatures and the GeoWaveInputFormat supplies data to the Hadoop RDD.

[NOTE]
====
GeoWaveInputFormat does not remove duplicate features that reference lines and/or polygons spanning multiple index regions. If working with duplication in time ranges, please take a look at the GeoWave https://github.com/locationtech/geowave/blob/master/core/mapreduce/src/main/java/mil/nga/giat/geowave/mapreduce/dedupe/GeoWaveDedupeJobRunner.java[GeoWaveDedupeJobRunner, window="_blank"].

It is also important to note that, while duplication can arise as an issue, though if using the XZ-Order SFC (which is used by default for lines and polygons), it does not require duplication. The only time there should be a duplication issue is if you are indexing a time range and the time range spans multiple periods based on your temporal binning strategy. The default periodicity is "YEAR," meaning that a row must be duplicate under default spatial-temporal indexing if you are trying to index a time range that crosses December 31 23:59:59.999 and January 1 00:00:00 of the following year.
====


The following algorithms are provided.

[width="100%",cols="2,10",options="header"]
|=========================================================
|Name |Description
|KMeans++| A K-Means implementation to find K centroids over the population of data. A set of preliminary sampling iterations find an optimal value of K and the initial set of K centroids. The algorithm produces K centroids and their associated polygons.  Each polygon represents the concave hull containing all features associated with a centroid. The algorithm supports drilling down multiple levels. At each level, the set centroids are determined from the set of features associated the same centroid from the previous level.
|KMeans Jump| Uses KMeans++ over a range of k, choosing an optimal k using an information theoretic based measurement.

Example Usage: _yarn jar geowave-tools.jar analytic kmeansjump_
|KMeans Parallel| Performs a KMeans Parallel Cluster

Example Usage: _yarn jar geowave-tools.jar analytic kmeansparallel_
|DBScan| The Density Based Scanner algorithm produces a set of convex polygons for each region meeting density criteria. Density of region is measured by a minimum cardinality of enclosed features within a specified distance from each other.

Example Usage: _yarn jar geowave-tools.jar analytic dbscan_
|Nearest Neighbors| An infrastructure component that produces all the neighbors of a feature within a specific distance.

Example Usage: _yarn jar geowave-tools.jar analytic nn_
|=========================================================

[NOTE]
====
Building/Developing GeoWave analytics is outside the scope of this document. For details around how to build the GeoWave source and tools, please refer to the link:devguide.html#building[GeoWave Developer Guide, window="_blank"].
====

=== Running
The GeoWave analytical tools are made available through MapReduce and Spark implementations. To run the tools, use the yarn or hadoop system APIs, as outlined below.
[source, bash]
----
yarn jar geowave-tools.jar analytic <algorithm> <options> <store>
----

The above command will execute <algorithm> (such as dbscan), sourcing the data from the <store> datastore (see config addstore).
