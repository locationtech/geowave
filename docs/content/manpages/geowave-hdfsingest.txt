//:geowave-hdfsingest(1)
//:=====================
//::doctype: manpage

NAME
//:----

geowave-hdfsingest - Load content from an HDFS file system

SYNOPSIS
//:--------

*geowave -hdfsingest* <options>

DESCRIPTION
//:-----------

The geowave -hdfsingest(1) operator first copies the local files to an Avro record in HDFS, then executes the
ingest process as a map-reduce job. Data is ingested into Geowave using the GeowaveInputFormat. This is likely to be
the fastest ingest method overall for data sets of any notable size (or if they have a large ingest/transform cost).

OPTIONS
//:-------

-b, --base <arg>::
Base input file or directory to crawl with one of the supported ingest types

-c, --clear::
Clear ALL data stored with the same prefix as this namespace (optional; default is to append data to the namespace if it exists)

-dim, --dimensionality <arg>::
The dimensionality type for the index, either 'spatial' or 'spatial-temporal' (optional; default is 'spatial')

-f,--formats <arg>::
Explicitly set the ingest formats by name (or multiple comma-delimited formats), if not set all available ingest formats will be used

-h, --help::
Display help

-hdfs <arg>::
HDFS hostname and port in the format hostname:port

-hdfsbase <arg>::
The fully qualified path to the base directory in HDFS

-i, --instance-id <arg>::
The Accumulo instance ID

-jobtracker <arg>::
Hadoop job tracker hostname and port in the format hostname:port

-l, --list::
List the available ingest types

-n, --namespace <arg>::
The table namespace (optional; default is no namespace)

-p, --password <arg>::
The password for the user

-resourceman <arg>::
YARN resource manager hostname and port in the format hostname:port

-u, --user <arg>::
A valid Accumulo user ID

-v, --visibility <arg>::
The visibility of the data ingested (optional; default is 'public')

-x, --extension <arg>::
Individual or comma-delimited set of file extensions to accept (optional)

-z, --zookeepers <arg>::
A comma-separated list of zookeeper servers that an Accumulo instance is using

ADDITIONAL
//:----------

The options here are, for the most part, same as for *geowave -localingest*, with a few additions.

The hdfs argument should be the hostname and port, so something like "hdfs-namenode.cluster1.com:8020".

The hdfsbase argument is the root path in hdfs that will serve as the base for the stage location. If the directory
doesn't exist it will be created. The actual ingest file will be created in a "type" (plugin type - seen with the --list
option) subdirectory under this base directory.

The jobtracker argument is the hostname and port for the jobtracker, so something like mapreduce-namenode.cluster1.com:8021

The hdfsstage and poststage options will just be subsets of this comment; the first creating an avro file in hdfs,
the second reading this avro file and ingesting into GeoWave
