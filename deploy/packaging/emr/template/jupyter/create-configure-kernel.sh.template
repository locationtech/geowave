
#-------------------------------------------------------------------------------
# Copyright (c) 2013-2018 Contributors to the Eclipse Foundation
# 
# See the NOTICE file distributed with this work for additional
# information regarding copyright ownership.
# All rights reserved. This program and the accompanying materials
# are made available under the terms of the Apache License,
# Version 2.0 which accompanies this distribution and is available at
# http://www.apache.org/licenses/LICENSE-2.0.txt
#-------------------------------------------------------------------------------
#!/bin/bash

GEOWAVE_VER=${1:-$GEOWAVE_VERSION_TOKEN}
PIXIEDUST_HOME=${2:-$HOME/pixiedust/}
CONDA_INSTALL=${3:-$HOME/conda/bin}
KERNEL_OUT=${4:-$HOME/.local/share/jupyter/kernels/}
SPARK_HOME=${5:-/usr/lib/spark}
MASTER_ARG=${6:-yarn}

INTIAL_POLLING_INTERVAL=15 # This gets doubled for each attempt up to max_attempts

KERNEL_DIR=$HOME/.local/share/jupyter/kernels/

# Avoid race conditions and actually poll for availability of component dependencies
# Credit: http://stackoverflow.com/questions/8350942/how-to-re-run-the-curl-command-automatically-when-the-error-occurs/8351489#8351489
with_backoff() {
  local max_attempts=${ATTEMPTS-5}
  local timeout=${INTIAL_POLLING_INTERVAL-1}
  local attempt=0
  local exitCode=0

  while (( $attempt < $max_attempts ))
  do
    set +e
    "$@"
    exitCode=$?
    set -e

    if [[ $exitCode == 0 ]]
    then
      break
    fi

    echo "Retrying $@ in $timeout.." 1>&2
    sleep $timeout
    attempt=$(( attempt + 1 ))
    timeout=$(( timeout * 2 ))
  done

  if [[ $exitCode != 0 ]]
  then
    echo "Fail: $@ failed to complete after $max_attempts attempts" 1>&2
  fi

  return $exitCode
}

is_spark_available() {
	pyspark --version /
	return $?
}

wait_until_spark_is_available() {
	with_backoff is_spark_available
	if [ $? != 0 ]; then
		echo "HDFS not available before timeout. Exiting ..."
		exit 1
	fi
}

#Install the Kernel
wait_until_spark_is_available

# Create the jupyter kernel
mkdir -p  ${PIXIEDUST_HOME}

${CONDA_INSTALL}/jupyter pixiedust install <<END
n
${PIXIEDUST_HOME}
n
${SPARK_HOME}
y
y
y
END

#Use jq to remove unnecessary keys
GEOWAVE_INSTALL=/usr/local/geowave/tools/geowave-tools-${GEOWAVE_VER}-apache.jar
PIXIEDUST_KERNELS=$(find $KERNEL_DIR -type d -name pythonwithpixiedustspark*)
echo ${PIXIEDUST_KERNELS}
KERNEL_JSON=$PIXIEDUST_KERNELS/kernel.json
jq 'del(.env["SPARK_LOCAL_IP"])' ${KERNEL_JSON} > tmp.$$.json && mv tmp.$$.json ${KERNEL_JSON}
jq 'del(.env["SPARK_DRIVER_MEMORY"])' ${KERNEL_JSON} > tmp.$$.json && mv tmp.$$.json ${KERNEL_JSON}

#Disable shell file globbing
set -f

#Use jq to read submit args into array
submit_args=($(jq -r '.env["PYSPARK_SUBMIT_ARGS"]' ${KERNEL_JSON}))

#Enable shell file globbing
set +f

#Add geowave jar to submit --jars option
submit_args[1]=${submit_args[1]},${GEOWAVE_INSTALL}

#Modify master to use yarn/local
submit_args[5]=${MASTER_ARG}

#Pulling array out to string so it can be passed properly to jq
submit_string=${submit_args[@]}

#Modifying default spark allocation properties to use max memory resources available with HBase
YARN_SCHED_MAX=`xmllint --xpath 'string(//property[name="yarn.scheduler.maximum-allocation-mb"]/value)' /etc/hadoop/conf/yarn-site.xml`
YARN_CONT_MAX=`xmllint --xpath 'string(//property[name="yarn.nodemanager.resource.memory-mb"]/value)' /etc/hadoop/conf/yarn-site.xml`
echo "Yarn Scheduler Max Memory = ${YARN_SCHED_MAX}(MB)"
echo "Yarn Container Max Memory = ${YARN_CONT_MAX}(MB)"

MAX_MOD=0.9
CONT_MOD=0.8
#Use bc calculator to get new max and container memory and truncate floating result
MOD_SCHED_MAX=$(echo "($YARN_SCHED_MAX*$MAX_MOD) / 1" | bc)
MOD_CONT_MAX=$(echo "($YARN_CONT_MAX*$CONT_MOD) / 1" | bc)

echo "Modified Yarn Scheduler Max Memory = ${MOD_SCHED_MAX}(MB)"
echo "Modified Yarn Container Max Memory = ${MOD_CONT_MAX}(MB)"

DRIVER_MEM="--driver-memory ${MOD_SCHED_MAX}M "
EXECUTOR_MEM="--executor-memory ${MOD_CONT_MAX}M "

submit_string=${DRIVER_MEM}${EXECUTOR_MEM}${submit_string}

echo "New Spark Submit Options: ${submit_string}"

#Write the new submit_args to the kernel.json
jq --arg submit_args "${submit_string}" '.env["PYSPARK_SUBMIT_ARGS"]= $submit_args' ${KERNEL_JSON} > tmp.$$.json && mv tmp.$$.json ${KERNEL_JSON}

echo "Modified Kernel to use yarn by default"

# Copy final modified kernel to output install location
cp -R ${PIXIEDUST_KERNELS} ${KERNEL_OUT}
