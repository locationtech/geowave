{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pixiedust\n",
    "Start by importing pixiedust which if all bootstrap and install steps were run correctly.\n",
    "You should see below for opening the pixiedust database successfully with no errors.\n",
    "Depending on the version of pixiedust that gets installed, it may ask you to update.\n",
    "If so, run this first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user --upgrade pixiedust\n",
    "#!pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"pm_overallContainer987ff63f\" style=\"display:none\">\n",
       "    <table width=\"100%\" style=\"border:0px\">\n",
       "        <tr style=\"border:0px\">\n",
       "            <td width=\"20px\" style=\"border:0px\"><span id=\"twistie987ff63f\" style=\"color:blue;font-size:x-large;cursor:pointer\">&#9656;</span></td>\n",
       "            <td width=\"130px\" style=\"text-align:left;border:0px\"><span id=\"pm_overallJobName987ff63f\"></span>:</td>\n",
       "            <td width=\"calc(100% - 150px)\" style=\"border:0px\"><progress id=\"pm_overallProgress987ff63f\" max=\"100\" value=\"0\" style=\"width:100%\"></progress></td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</div>\n",
       "<div id=\"pm_container987ff63f\" style=\"display:none\">\n",
       "    <ul class=\"nav nav-tabs\" id=\"progressMonitors987ff63f\">\n",
       "    </ul>\n",
       "    <div class=\"tab-content\" id=\"tabContent987ff63f\">\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<script>\n",
       "$(\"#twistie987ff63f\").click(function(){\n",
       "    visible = $(\"#pm_container987ff63f\").is(':visible');\n",
       "    $(\"#pm_container987ff63f\").slideToggle(\"slow\");\n",
       "    $(this).html(visible?\"&#9656;\":\"&#9662;\")\n",
       "});\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"pm_overallContainer9bb02e98\" style=\"display:none\">\n",
       "    <table width=\"100%\" style=\"border:0px\">\n",
       "        <tr style=\"border:0px\">\n",
       "            <td width=\"20px\" style=\"border:0px\"><span id=\"twistie9bb02e98\" style=\"color:blue;font-size:x-large;cursor:pointer\">&#9656;</span></td>\n",
       "            <td width=\"130px\" style=\"text-align:left;border:0px\"><span id=\"pm_overallJobName9bb02e98\"></span>:</td>\n",
       "            <td width=\"calc(100% - 150px)\" style=\"border:0px\"><progress id=\"pm_overallProgress9bb02e98\" max=\"100\" value=\"0\" style=\"width:100%\"></progress></td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</div>\n",
       "<div id=\"pm_container9bb02e98\" style=\"display:none\">\n",
       "    <ul class=\"nav nav-tabs\" id=\"progressMonitors9bb02e98\">\n",
       "    </ul>\n",
       "    <div class=\"tab-content\" id=\"tabContent9bb02e98\">\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<script>\n",
       "$(\"#twistie9bb02e98\").click(function(){\n",
       "    visible = $(\"#pm_container9bb02e98\").is(':visible');\n",
       "    $(\"#pm_container9bb02e98\").slideToggle(\"slow\");\n",
       "    $(this).html(visible?\"&#9656;\":\"&#9662;\")\n",
       "});\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Job Progress Monitor already enabled\n"
     ]
    }
   ],
   "source": [
    "pixiedust.enableJobMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the SQLContext and inspecting pyspark Context\n",
    "Pixiedust imports pyspark and the SparkContext + SparkSession should be already available through the \"sc\" and \"spark\" variables respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"pm_overallContainerf7b09cd5\" style=\"display:none\">\n",
       "    <table width=\"100%\" style=\"border:0px\">\n",
       "        <tr style=\"border:0px\">\n",
       "            <td width=\"20px\" style=\"border:0px\"><span id=\"twistief7b09cd5\" style=\"color:blue;font-size:x-large;cursor:pointer\">&#9656;</span></td>\n",
       "            <td width=\"130px\" style=\"text-align:left;border:0px\"><span id=\"pm_overallJobNamef7b09cd5\"></span>:</td>\n",
       "            <td width=\"calc(100% - 150px)\" style=\"border:0px\"><progress id=\"pm_overallProgressf7b09cd5\" max=\"100\" value=\"0\" style=\"width:100%\"></progress></td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</div>\n",
       "<div id=\"pm_containerf7b09cd5\" style=\"display:none\">\n",
       "    <ul class=\"nav nav-tabs\" id=\"progressMonitorsf7b09cd5\">\n",
       "    </ul>\n",
       "    <div class=\"tab-content\" id=\"tabContentf7b09cd5\">\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<script>\n",
       "$(\"#twistief7b09cd5\").click(function(){\n",
       "    visible = $(\"#pm_containerf7b09cd5\").is(':visible');\n",
       "    $(\"#pm_containerf7b09cd5\").slideToggle(\"slow\");\n",
       "    $(this).html(visible?\"&#9656;\":\"&#9662;\")\n",
       "});\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 2.2.0\n",
      "Python Version: 3.5\n",
      "Application Name: pyspark-shell\n",
      "Application ID: application_1509968683912_0002\n",
      "Spark Master: yarn\n"
     ]
    }
   ],
   "source": [
    "# Print Spark info and create sql_context\n",
    "print('Spark Version: {0}'.format(sc.version))\n",
    "print('Python Version: {0}'.format(sc.pythonVer))\n",
    "print('Application Name: {0}'.format(sc.appName))\n",
    "print('Application ID: {0}'.format(sc.applicationId))\n",
    "print('Spark Master: {0}'.format( sc.master))\n",
    "\n",
    "sql_context = SQLContext(sc, sparkSession=spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and ingest the GPX data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"pm_overallContainer6ea3f4af\" style=\"display:none\">\n",
       "    <table width=\"100%\" style=\"border:0px\">\n",
       "        <tr style=\"border:0px\">\n",
       "            <td width=\"20px\" style=\"border:0px\"><span id=\"twistie6ea3f4af\" style=\"color:blue;font-size:x-large;cursor:pointer\">&#9656;</span></td>\n",
       "            <td width=\"130px\" style=\"text-align:left;border:0px\"><span id=\"pm_overallJobName6ea3f4af\"></span>:</td>\n",
       "            <td width=\"calc(100% - 150px)\" style=\"border:0px\"><progress id=\"pm_overallProgress6ea3f4af\" max=\"100\" value=\"0\" style=\"width:100%\"></progress></td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</div>\n",
       "<div id=\"pm_container6ea3f4af\" style=\"display:none\">\n",
       "    <ul class=\"nav nav-tabs\" id=\"progressMonitors6ea3f4af\">\n",
       "    </ul>\n",
       "    <div class=\"tab-content\" id=\"tabContent6ea3f4af\">\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<script>\n",
       "$(\"#twistie6ea3f4af\").click(function(){\n",
       "    visible = $(\"#pm_container6ea3f4af\").is(':visible');\n",
       "    $(\"#pm_container6ea3f4af\").slideToggle(\"slow\");\n",
       "    $(this).html(visible?\"&#9656;\":\"&#9662;\")\n",
       "});\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17/11/06 12:30:10 INFO s3distcp.S3DistCp: Running with args: -libjars /usr/share/aws/emr/s3-dist-cp/lib/guava-15.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp-2.6.0.jar,/usr/share/aws/emr/s3-dist-cp/lib/s3-dist-cp.jar -D mapreduce.task.timeout=60000000 --src=s3://geowave-gpx-data/gpx --dest=hdfs://ip-10-0-0-224:8020/tmp/ \n",
      "17/11/06 12:30:10 INFO s3distcp.S3DistCp: S3DistCp args: --src=s3://geowave-gpx-data/gpx --dest=hdfs://ip-10-0-0-224:8020/tmp/ \n",
      "17/11/06 12:30:10 INFO s3distcp.S3DistCp: Using output path 'hdfs:/tmp/b098455b-de8c-4a2e-a0ed-6a7185672ba3/output'\n",
      "17/11/06 12:30:10 INFO s3distcp.S3DistCp: GET http://169.254.169.254/latest/meta-data/placement/availability-zone result: us-east-1f\n",
      "17/11/06 12:30:13 INFO s3distcp.FileInfoListing: Opening new file: hdfs:/tmp/b098455b-de8c-4a2e-a0ed-6a7185672ba3/files/1\n",
      "17/11/06 12:30:13 INFO s3distcp.S3DistCp: Created 1 files to copy 66 files \n",
      "17/11/06 12:30:13 INFO s3distcp.S3DistCp: Reducer number: 31\n",
      "17/11/06 12:30:14 INFO impl.TimelineClientImpl: Timeline service address: http://ip-10-0-0-224.ec2.internal:8188/ws/v1/timeline/\n",
      "17/11/06 12:30:14 INFO client.RMProxy: Connecting to ResourceManager at ip-10-0-0-224.ec2.internal/10.0.0.224:8032\n",
      "17/11/06 12:30:14 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "17/11/06 12:30:14 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "17/11/06 12:30:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1509968683912_0009\n",
      "17/11/06 12:30:14 INFO impl.YarnClientImpl: Submitted application application_1509968683912_0009\n",
      "17/11/06 12:30:14 INFO mapreduce.Job: The url to track the job: http://ip-10-0-0-224.ec2.internal:20888/proxy/application_1509968683912_0009/\n",
      "17/11/06 12:30:14 INFO mapreduce.Job: Running job: job_1509968683912_0009\n",
      "17/11/06 12:30:19 INFO mapreduce.Job: Job job_1509968683912_0009 running in uber mode : false\n",
      "17/11/06 12:30:19 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/11/06 12:30:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/11/06 12:30:28 INFO mapreduce.Job:  map 100% reduce 13%\n",
      "17/11/06 12:30:30 INFO mapreduce.Job:  map 100% reduce 16%\n",
      "17/11/06 12:30:31 INFO mapreduce.Job:  map 100% reduce 26%\n",
      "17/11/06 12:30:31 INFO mapreduce.Job: Task Id : attempt_1509968683912_0009_r_000014_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Reducer task failed to copy 1 files: s3://geowave-gpx-data/gpx/metadata etc\n",
      "\tat com.amazon.elasticmapreduce.s3distcp.CopyFilesReducer.cleanup(CopyFilesReducer.java:67)\n",
      "\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:635)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/11/06 12:30:32 INFO mapreduce.Job:  map 100% reduce 39%\n",
      "17/11/06 12:30:33 INFO mapreduce.Job:  map 100% reduce 45%\n",
      "17/11/06 12:30:34 INFO mapreduce.Job:  map 100% reduce 48%\n",
      "17/11/06 12:30:34 INFO mapreduce.Job: Task Id : attempt_1509968683912_0009_r_000020_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Reducer task failed to copy 1 files: s3://geowave-gpx-data/gpx/spatial etc\n",
      "\tat com.amazon.elasticmapreduce.s3distcp.CopyFilesReducer.cleanup(CopyFilesReducer.java:67)\n",
      "\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:635)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "17/11/06 12:30:35 INFO mapreduce.Job:  map 100% reduce 64%\n",
      "17/11/06 12:30:36 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "17/11/06 12:30:36 INFO mapreduce.Job: Task Id : attempt_1509968683912_0009_r_000014_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Reducer task failed to copy 1 files: s3://geowave-gpx-data/gpx/metadata etc\n",
      "\tat com.amazon.elasticmapreduce.s3distcp.CopyFilesReducer.cleanup(CopyFilesReducer.java:67)\n",
      "\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:635)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/11/06 12:30:37 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "17/11/06 12:30:38 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "17/11/06 12:30:39 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "17/11/06 12:30:39 INFO mapreduce.Job: Task Id : attempt_1509968683912_0009_r_000020_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Reducer task failed to copy 1 files: s3://geowave-gpx-data/gpx/spatial etc\n",
      "\tat com.amazon.elasticmapreduce.s3distcp.CopyFilesReducer.cleanup(CopyFilesReducer.java:67)\n",
      "\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:635)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "17/11/06 12:30:40 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "17/11/06 12:30:41 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "17/11/06 12:30:42 INFO mapreduce.Job: Task Id : attempt_1509968683912_0009_r_000014_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Reducer task failed to copy 1 files: s3://geowave-gpx-data/gpx/metadata etc\n",
      "\tat com.amazon.elasticmapreduce.s3distcp.CopyFilesReducer.cleanup(CopyFilesReducer.java:67)\n",
      "\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:635)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "17/11/06 12:30:45 INFO mapreduce.Job: Task Id : attempt_1509968683912_0009_r_000020_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: Reducer task failed to copy 1 files: s3://geowave-gpx-data/gpx/spatial etc\n",
      "\tat com.amazon.elasticmapreduce.s3distcp.CopyFilesReducer.cleanup(CopyFilesReducer.java:67)\n",
      "\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:635)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "Container killed by the ApplicationMaster.\n",
      "Container killed on request. Exit code is 143\n",
      "Container exited with a non-zero exit code 143\n",
      "\n",
      "17/11/06 12:30:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/11/06 12:30:48 INFO mapreduce.Job: Job job_1509968683912_0009 failed with state FAILED due to: Task failed task_1509968683912_0009_r_000014\n",
      "Job failed as tasks failed. failedMaps:0 failedReduces:1\n",
      "\n",
      "17/11/06 12:30:48 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1095\n",
      "\t\tFILE: Number of bytes written=3542047\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8776\n",
      "\t\tHDFS: Number of bytes written=68834511\n",
      "\t\tHDFS: Number of read operations=94\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=58\n",
      "\t\tS3: Number of bytes read=68834511\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tFailed reduce tasks=7\n",
      "\t\tKilled reduce tasks=4\n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=36\n",
      "\t\tRack-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=98400\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16292832\n",
      "\t\tTotal time spent by all map tasks (ms)=2050\n",
      "\t\tTotal time spent by all reduce tasks (ms)=169717\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2050\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=169717\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3148800\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=521370624\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=66\n",
      "\t\tMap output records=66\n",
      "\t\tMap output bytes=9027\n",
      "\t\tMap output materialized bytes=2970\n",
      "\t\tInput split bytes=152\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=991\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=72\n",
      "\t\tShuffled Maps =26\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=26\n",
      "\t\tGC time elapsed (ms)=2848\n",
      "\t\tCPU time spent (ms)=53140\n",
      "\t\tPhysical memory (bytes) snapshot=8480456704\n",
      "\t\tVirtual memory (bytes) snapshot=124262039552\n",
      "\t\tTotal committed heap usage (bytes)=13554417664\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8624\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "17/11/06 12:30:48 INFO s3distcp.S3DistCp: Try to recursively delete hdfs:/tmp/b098455b-de8c-4a2e-a0ed-6a7185672ba3/tempspace\n",
      "17/11/06 12:30:48 ERROR s3distcp.S3DistCp: The MapReduce job failed: Task failed task_1509968683912_0009_r_000014\n",
      "Job failed as tasks failed. failedMaps:0 failedReduces:1\n",
      "\n",
      "Exception in thread \"main\" java.lang.RuntimeException: The MapReduce job failed: Task failed task_1509968683912_0009_r_000014\n",
      "Job failed as tasks failed. failedMaps:0 failedReduces:1\n",
      "\n",
      "\tat com.amazon.elasticmapreduce.s3distcp.S3DistCp.run(S3DistCp.java:810)\n",
      "\tat com.amazon.elasticmapreduce.s3distcp.S3DistCp.run(S3DistCp.java:600)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n",
      "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n",
      "\tat com.amazon.elasticmapreduce.s3distcp.Main.main(Main.java:22)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "s3-dist-cp -D mapreduce.task.timeout=60000000 --src=s3://geowave-gpx-data/gpx --dest=hdfs://$HOSTNAME:8020/tmp/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"pm_overallContainer6c9d0950\" style=\"display:none\">\n",
       "    <table width=\"100%\" style=\"border:0px\">\n",
       "        <tr style=\"border:0px\">\n",
       "            <td width=\"20px\" style=\"border:0px\"><span id=\"twistie6c9d0950\" style=\"color:blue;font-size:x-large;cursor:pointer\">&#9656;</span></td>\n",
       "            <td width=\"130px\" style=\"text-align:left;border:0px\"><span id=\"pm_overallJobName6c9d0950\"></span>:</td>\n",
       "            <td width=\"calc(100% - 150px)\" style=\"border:0px\"><progress id=\"pm_overallProgress6c9d0950\" max=\"100\" value=\"0\" style=\"width:100%\"></progress></td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</div>\n",
       "<div id=\"pm_container6c9d0950\" style=\"display:none\">\n",
       "    <ul class=\"nav nav-tabs\" id=\"progressMonitors6c9d0950\">\n",
       "    </ul>\n",
       "    <div class=\"tab-content\" id=\"tabContent6c9d0950\">\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<script>\n",
       "$(\"#twistie6c9d0950\").click(function(){\n",
       "    visible = $(\"#pm_container6c9d0950\").is(':visible');\n",
       "    $(\"#pm_container6c9d0950\").slideToggle(\"slow\");\n",
       "    $(this).html(visible?\"&#9656;\":\"&#9662;\")\n",
       "});\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-06 12:31:22,576 [conf.ConfigSanityCheck] WARN : Use of instance.dfs.uri and instance.dfs.dir are deprecated. Consider using instance.volumes instead.\n",
      "2017-11-06 12:31:23,401 [htrace.SpanReceiverBuilder] ERROR: SpanReceiverBuilder cannot find SpanReceiver class org.apache.accumulo.tracer.ZooTraceClient: disabling span receiver.\n",
      "2017-11-06 12:31:23,401 [trace.DistributedTrace] WARN : Failed to load SpanReceiver org.apache.accumulo.tracer.ZooTraceClient\n",
      "2017-11-06 12:31:23,989 [shell.Shell] ERROR: org.apache.accumulo.core.client.AccumuloException: File C00001tr.rf does not exist in import dir\n",
      "2017-11-06 12:31:25,051 [conf.ConfigSanityCheck] WARN : Use of instance.dfs.uri and instance.dfs.dir are deprecated. Consider using instance.volumes instead.\n",
      "2017-11-06 12:31:25,774 [htrace.SpanReceiverBuilder] ERROR: SpanReceiverBuilder cannot find SpanReceiver class org.apache.accumulo.tracer.ZooTraceClient: disabling span receiver.\n",
      "2017-11-06 12:31:25,774 [trace.DistributedTrace] WARN : Failed to load SpanReceiver org.apache.accumulo.tracer.ZooTraceClient\n",
      "2017-11-06 12:31:26,115 [impl.TableOperationsImpl] INFO : Imported table sets 'table.iterator.minc.STATS_COMBINER' to '10,mil.nga.giat.geowave.datastore.accumulo.MergingCombiner'.  Ensure this class is on Accumulo classpath.\n",
      "2017-11-06 12:31:26,115 [impl.TableOperationsImpl] INFO : Imported table sets 'table.iterator.majc.STATS_COMBINER' to '10,mil.nga.giat.geowave.datastore.accumulo.MergingCombiner'.  Ensure this class is on Accumulo classpath.\n",
      "2017-11-06 12:31:26,115 [impl.TableOperationsImpl] INFO : Imported table sets 'table.iterator.scan.STATS_COMBINER' to '10,mil.nga.giat.geowave.datastore.accumulo.MergingCombiner'.  Ensure this class is on Accumulo classpath.\n",
      "2017-11-06 12:31:26,701 [shell.Shell] ERROR: org.apache.accumulo.core.client.AccumuloException: Error renaming files Permission denied: user=accumulo, access=WRITE, inode=\"/tmp/metadata/A00001qw.rf\":hadoop:hadoop:drwxr-xr-x\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:320)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:216)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1728)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameTo(FSDirRenameOp.java:459)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameToInt(FSDirRenameOp.java:73)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameTo(FSNamesystem.java:3652)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rename(NameNodeRpcServer.java:868)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rename(ClientNamenodeProtocolServerSideTranslatorPB.java:575)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2045)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/accumulo-1.8.1/lib/slf4j-log4j12.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/accumulo-1.8.1/lib/slf4j-log4j12.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/opt/accumulo/bin/accumulo shell -u root -p secret -e \"importtable geowave.germany_gpx_SPATIAL_IDX /tmp/spatial\"\n",
    "/opt/accumulo/bin/accumulo shell -u root -p secret -e \"importtable geowave.germany_gpx_GEOWAVE_METADATA /tmp/metadata\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Datastores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# clear out potential old runs\n",
    "geowave config rmstore kmeans_gpx\n",
    "geowave config rmstore germany_gpx_accumulo\n",
    "\n",
    "# configure geowave connection params for name stores \"germany_gpx_accumulo\" and \"kmeans_hbase\"\n",
    "geowave config addstore germany_gpx_accumulo --gwNamespace geowave.germany_gpx -t accumulo --zookeeper $HOSTNAME:2181 --instance accumulo --user root --password secret\n",
    "geowave config addstore kmeans_gpx --gwNamespace geowave.kmeans -t hbase --zookeeper $HOSTNAME:2181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "geowave remote clear kmeans_gpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab classes from jvm\n",
    "hbase_options_class = sc._jvm.mil.nga.giat.geowave.datastore.hbase.operations.config.HBaseRequiredOptions\n",
    "accumulo_options_class = sc._jvm.mil.nga.giat.geowave.datastore.accumulo.operations.config.AccumuloRequiredOptions\n",
    "kmeans_runner_class = sc._jvm.mil.nga.giat.geowave.analytic.javaspark.kmeans.KMeansRunner\n",
    "query_options_class = sc._jvm.mil.nga.giat.geowave.core.store.query.QueryOptions\n",
    "geowave_rdd_class = sc._jvm.mil.nga.giat.geowave.analytic.javaspark.GeoWaveRDD\n",
    "sf_df_class = sc._jvm.mil.nga.giat.geowave.analytic.javaspark.sparksql.SimpleFeatureDataFrame\n",
    "byte_array_class = sc._jvm.mil.nga.giat.geowave.core.index.ByteArrayId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup input datastore\n",
    "input_store = accumulo_options_class()\n",
    "input_store.setInstance('accumulo')\n",
    "input_store.setUser('root')\n",
    "input_store.setPassword('secret')\n",
    "input_store.setZookeeper(os.environ['HOSTNAME'] + ':2181')\n",
    "input_store.setGeowaveNamespace('geowave.germany_gpx')\n",
    "\n",
    "#Setup output datastore\n",
    "output_store = hbase_options_class()\n",
    "output_store.setZookeeper(os.environ['HOSTNAME'] + ':2181')\n",
    "output_store.setGeowaveNamespace('geowave.kmeans')\n",
    "\n",
    "#Create a instance of the runner\n",
    "kmeans_runner = kmeans_runner_class()\n",
    "\n",
    "input_store_plugin = input_store.createPluginOptions()\n",
    "output_store_plugin = output_store.createPluginOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the appropriate properties\n",
    "#We want it to execute using the existing JavaSparkContext wrapped by python.\n",
    "kmeans_runner.setJavaSparkContext(sc._jsc)\n",
    "\n",
    "kmeans_runner.setAdapterId('gpxpoint')\n",
    "kmeans_runner.setNumClusters(8)\n",
    "kmeans_runner.setInputDataStore(input_store_plugin)\n",
    "kmeans_runner.setOutputDataStore(output_store_plugin)\n",
    "kmeans_runner.setCqlFilter(\"BBOX(geometry,  13.3, 52.45, 13.5, 52.5)\")\n",
    "kmeans_runner.setCentroidTypeName('mycentroids')\n",
    "kmeans_runner.setHullTypeName('myhulls')\n",
    "kmeans_runner.setGenerateHulls(True)\n",
    "kmeans_runner.setComputeHullData(True)\n",
    "#execute the kmeans runner\n",
    "kmeans_runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Centroids into DataFrame and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create the dataframe and get a rdd for the output of kmeans\n",
    "sf_df = sf_df_class(spark._jsparkSession)\n",
    "adapter_id = byte_array_class('mycentroids')\n",
    "\n",
    "queryOptions = None\n",
    "adapterIt = output_store_plugin.createAdapterStore().getAdapters()\n",
    "adapterForQuery = None\n",
    "while (adapterIt.hasNext()):\n",
    "    adapter = adapterIt.next()\n",
    "    if (adapter.getAdapterId().equals(adapter_id)):\n",
    "        adapterForQuery = adapter\n",
    "        queryOptions = query_options_class(adapterForQuery)\n",
    "        break\n",
    "\n",
    "output_rdd = geowave_rdd_class.rddForSimpleFeatures(sc._jsc.sc(), output_store_plugin, None, queryOptions)\n",
    "\n",
    "sf_df.init(output_store_plugin, adapter_id)\n",
    "\n",
    "df = sf_df.getDataFrame(output_rdd)\n",
    "# Convert Java DataFrame to Python DataFrame\n",
    "import pyspark.mllib.common as convert\n",
    "py_df = convert._java2py(sc, df)\n",
    "\n",
    "py_df.createOrReplaceTempView('mycentroids')\n",
    "\n",
    "df = sql_context.sql(\"select * from mycentroids\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse DataFrame data into lat/lon columns and display centroids on map\n",
    "Using pixiedust's built in map visualization we can display data on a map assuming it has the following properties.\n",
    "- Keys: put your latitude and longitude fields here. They must be floating values. These fields must be named latitude, lat or y and longitude, lon or x.\n",
    "- Values: the field you want to use to thematically color the map. Only one field can be used.\n",
    "\n",
    "Also you will need a access token from whichever map renderer you choose to use with pixiedust (mapbox, google).\n",
    "Follow the instructions in the token help on how to create and use the access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "coloropacity": "89",
      "handlerId": "mapView",
      "keyFields": "lat,lon",
      "mapboxtoken": "pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4M29iazA2Z2gycXA4N2pmbDZmangifQ.-g_vE53SD2WrJ6tFX7QHmA",
      "rowCount": "500",
      "title": "Centroids",
      "valueFields": "ClusterIndex"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert the string point information into lat long columns and create a new dataframe for those.\n",
    "import pyspark\n",
    "def parseRow(row):\n",
    "    lat_start = row.geom.rfind(' ') + 1\n",
    "    lat_end = row.geom.rfind(')')\n",
    "    lat = row.geom[lat_start:lat_end]\n",
    "    lon_start = row.geom.find('(') + 1\n",
    "    lon_end = row.geom.rfind(' ', lon_start)\n",
    "    lon = row.geom[lon_start:lon_end]\n",
    "    return pyspark.sql.Row(lat=float(lat), lon=float(lon), ClusterIndex=row.ClusterIndex)\n",
    "    \n",
    "row_rdd = df.rdd\n",
    "new_rdd = row_rdd.map(lambda row: parseRow(row))\n",
    "new_df =new_rdd.toDF() \n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export KMeans Hulls to DataFrame\n",
    "If you have some more complex data to visualize pixiedust may not be the best option.\n",
    "\n",
    "The Kmeans hull generation outputs polygons that would be difficult for pixiedust to display without\n",
    "creating a special plugin. \n",
    "\n",
    "Instead, we can use another map renderer to visualize our data. For the Kmeans hulls we will use ipyleaflet to visualize the data. We will start by grabbing the results for the hull generation and putting them into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create the dataframe and get a rdd for the output of kmeans\n",
    "sf_df_hulls = sf_df_class(spark._jsparkSession)\n",
    "adapter_id = byte_array_class('myhulls')\n",
    "\n",
    "queryOptions = None\n",
    "adapterIt = output_store_plugin.createAdapterStore().getAdapters()\n",
    "adapterForQuery = None\n",
    "while (adapterIt.hasNext()):\n",
    "    adapter = adapterIt.next()\n",
    "    if (adapter.getAdapterId().equals(adapter_id)):\n",
    "        adapterForQuery = adapter\n",
    "        queryOptions = query_options_class(adapterForQuery)\n",
    "        break\n",
    "\n",
    "output_rdd_hulls = geowave_rdd_class.rddForSimpleFeatures(sc._jsc.sc(), output_store_plugin, None, queryOptions)\n",
    "\n",
    "sf_df_hulls.init(output_store_plugin, adapter_id)\n",
    "\n",
    "df_hulls = sf_df_hulls.getDataFrame(output_rdd_hulls)\n",
    "# Convert Java DataFrame to Python DataFrame\n",
    "import pyspark.mllib.common as convert\n",
    "py_df_hulls = convert._java2py(sc, df_hulls)\n",
    "\n",
    "py_df_hulls.createOrReplaceTempView('myhulls')\n",
    "\n",
    "df_hulls = sql_context.sql(\"select * from myhulls order by Density\")\n",
    "\n",
    "display(df_hulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Kmeans hull results to geojson\n",
    "ipyleaflet provides an easy way to visualize leaflet maps in jupyter notebooks.\n",
    "\n",
    "Our hull data contains wkt geometry strings that we will use with a small python library to convert the geometry to GeoJson. Once our data is converted to a proper GeoJson feature collection we can use ipyleaflet to easily load and display that data on a map.\n",
    "\n",
    "For more information on the GeoJson format visit: http://geojson.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geomet import wkt\n",
    "from ipyleaflet import (\n",
    "    Map,\n",
    "    Marker,\n",
    "    TileLayer, ImageOverlay,\n",
    "    Polyline, Polygon, Rectangle, Circle, CircleMarker,\n",
    "    GeoJSON,\n",
    "    DrawControl\n",
    ")\n",
    "\n",
    "# Collecting the results will give a array of Rows.\n",
    "hulls_results = df_hulls.collect()\n",
    "hulls_geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": []\n",
    "}\n",
    "for hull in hulls_results:\n",
    "    hull = hull.asDict(True)\n",
    "    output_geojson = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {},\n",
    "        \"properties\": {}\n",
    "    }\n",
    "    # Convert geometry to geojson with geomet\n",
    "    geom = wkt.loads(hull[\"geom\"])\n",
    "    output_geojson[\"geometry\"] = geom\n",
    "    for propKey in hull:\n",
    "        if propKey != \"geom\":\n",
    "            output_geojson[\"properties\"][propKey] = hull[propKey]\n",
    "    hulls_geojson[\"features\"].append(output_geojson)\n",
    "print(\"Count: {0} Features\".format(len(hulls_geojson[\"features\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = [52.54, 13.49]\n",
    "zoom = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Map(center=center, zoom=zoom)\n",
    "g = GeoJSON(data=hulls_geojson)\n",
    "m.add_layer(g)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.2)",
   "language": "python",
   "name": "pythonwithpixiedustspark22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
